{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNrHXi4OfredeU5WqA3+kip",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/veldahung/huggingface_pipeline/blob/main/huggingface_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e1GhJ0l_3xRc"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentiment analysis example\n",
        "sentiment_analyzer = pipeline(\"sentiment-analysis\")\n",
        "#classifier=pipeline(task=\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kurBcZ1L3ybu",
        "outputId": "e4701952-278f-4517-8fd0-51971425779c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classifier=pipeline(task=\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")"
      ],
      "metadata": {
        "id": "O9LWtQtC4Uvi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"I'm really enjoying Colab for exploring NLP!\""
      ],
      "metadata": {
        "id": "2utjFa3a35Y_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentiment analysis example\n",
        "result = sentiment_analyzer(input_text)\n",
        "\n",
        "classifier_result=classifier(input_text)"
      ],
      "metadata": {
        "id": "zLAhnW8a38Cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UDocfN-m4NKI",
        "outputId": "c8799f56-f0e7-4cf3-eebc-ae2167a017fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'label': 'POSITIVE', 'score': 0.9998200535774231}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(classifier_result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GV7EssOl5Hyx",
        "outputId": "c573c1a4-8dda-4822-a400-4b96d2135cc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'label': 'POSITIVE', 'score': 0.9998200535774231}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classifier=pipeline(task=\"sentiment-analysis\", model=\"siebert/sentiment-roberta-large-english\")"
      ],
      "metadata": {
        "id": "FeRpeqm-6-zl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier_result=classifier(input_text)\n",
        "print(classifier_result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NhuNRU-67MRd",
        "outputId": "e63d3e76-bcbd-4613-ce5c-24a277e40857"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'label': 'POSITIVE', 'score': 0.9988937973976135}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")"
      ],
      "metadata": {
        "id": "fjGRdE-17kAR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "text = [\"Hugging Face is a company and platform specializing in natural language processing (NLP) and artificial intelligence (AI). They are renowned for their contributions to the development and distribution of state-of-the-art NLP models and tools. Hugging Face offers a comprehensive library known as 'Transformers,' which provides easy access to a wide range of pre-trained language models, facilitating the implementation of cutting-edge NLP applications. Additionally, the company promotes community collaboration by fostering an open-source environment, allowing developers and researchers to share and contribute to advancements in the field of NLP. Hugging Face has become a prominent hub for AI enthusiasts seeking powerful and efficient tools for language-related tasks.\"]\n",
        "\n",
        "summarized_text = summarizer(text, min_length=5, max_length=140)[0]['summary_text']\n",
        "\n",
        "print(summarized_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aGoz8gNA9qKh",
        "outputId": "e8c24282-c73f-4adb-b08c-17ca27b7a25d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hugging Face is a company and platform specializing in natural language processing (NLP) and artificial intelligence (AI) They are renowned for their contributions to the development and distribution of state-of-the-art NLP models and tools. The company promotes community collaboration by fostering an open-source environment.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Input:**\n",
        "\n",
        "Hugging Face is a company and platform specializing in natural language processing (NLP) and artificial intelligence (AI). They are renowned for their contributions to the development and distribution of state-of-the-art NLP models and tools. Hugging Face offers a comprehensive library known as 'Transformers,' which provides easy access to a wide range of pre-trained language models, facilitating the implementation of cutting-edge NLP applications. Additionally, the company promotes community collaboration by fostering an open-source environment, allowing developers and researchers to share and contribute to advancements in the field of NLP. Hugging Face has become a prominent hub for AI enthusiasts seeking powerful and efficient tools for language-related tasks.\n",
        "\n",
        "### **Output:**\n",
        "\n",
        "Hugging Face is a company and platform specializing in natural language processing (NLP) and artificial intelligence (AI) They are renowned for their contributions to the development and distribution of state-of-the-art NLP models and tools. The company promotes community collaboration by fostering an open-source environment."
      ],
      "metadata": {
        "id": "oMw-YuGy-7vu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **pipeline**\n",
        "\n",
        "The `pipeline` function abstracts away the complexity of loading models, tokenizing input, and processing output. It automatically uses an appropriate pre-trained model based on the specified task, making it a convenient tool for users who want to quickly apply pre-trained models to specific NLP tasks without having to deal with the underlying details.\n",
        "\n",
        "Here are some key points about pipelines:\n",
        "\n",
        "1. **Ease of Use:** Pipelines simplify the code needed to perform common NLP tasks, reducing the amount of boilerplate code required.\n",
        "\n",
        "2. **Automatic Model Selection:** Pipelines automatically select an appropriate pre-trained model for the specified task, so users don't need to manually choose and load models.\n",
        "\n",
        "3. **Task Agnostic:** The same pipeline interface can be used for various NLP tasks, making it versatile.\n",
        "\n",
        "4. **Default Settings:** Pipelines use default settings for tokenization and model configurations, making them suitable for users who prefer a quick and easy setup.\n",
        "\n",
        "While pipelines are convenient for many use cases, users who need more control over the tokenization process or specific model configurations might prefer using the lower-level interfaces provided by the Transformers library."
      ],
      "metadata": {
        "id": "NZcPn1qKcjmm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict, Counter\n",
        "import json\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "#This function, print_encoding, takes a dictionary model_inputs and prints its contents in a structured and indented format.\n",
        "def print_encoding(model_inputs, indent=4):\n",
        "    indent_str = \" \" * indent\n",
        "    print(\"{\")\n",
        "    for k, v in model_inputs.items():\n",
        "        print(indent_str + k + \":\") #Prints the key\n",
        "        print(indent_str + indent_str + str(v)) #Prints the value\n",
        "    print(\"}\")"
      ],
      "metadata": {
        "id": "f5XriGisdMTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# Initialize the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"siebert/sentiment-roberta-large-english\")\n",
        "# Initialize the model\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"siebert/sentiment-roberta-large-english\")"
      ],
      "metadata": {
        "id": "cJ3XJIaxcys1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Input text\n",
        "inputs = \"I don't want to learn about Hugging Face Transformers!\"\n",
        "\n",
        "# Tokenize the input using the specified tokenizer and convert to PyTorch tensors\n",
        "tokenized_inputs = tokenizer(inputs, return_tensors=\"pt\")\n",
        "\n",
        "# Forward pass through the model with the tokenized input\n",
        "outputs = model(**tokenized_inputs)\n",
        "\n",
        "# List of labels for classification (e.g., sentiment analysis)\n",
        "labels = ['NEGATIVE', 'POSITIVE']\n",
        "\n",
        "# Get the predicted class by selecting the index with the highest logit value\n",
        "# \"logits\" refer to the raw, unnormalized predictions that a model generates\n",
        "#Logits are the output of a neural network before it undergoes a softmax activation function.\n",
        "prediction = torch.argmax(outputs.logits)\n",
        "\n",
        "\n",
        "# Print the original input text\n",
        "print(\"Input:\")\n",
        "print(inputs)\n",
        "print()\n",
        "\n",
        "# Print the tokenized input\n",
        "print(\"Tokenized Inputs:\")\n",
        "print_encoding(tokenized_inputs)\n",
        "print()\n",
        "\n",
        "# Print the model outputs, which include logits and other information\n",
        "print(\"Model Outputs:\")\n",
        "print(outputs)\n",
        "print()\n",
        "#The loss field is None, indicating that no loss value was computed.\n",
        "#These logits are the model's unnormalized predictions for each class.\n",
        "#grad_fn: This attribute indicates the gradient function that was used to compute the tensor.\n",
        "#Hidden states are intermediate representations of the input sequence at different layers of the model.\n",
        "#Attention weights show how much attention the model is giving to different parts of the input sequence.\n",
        "\n",
        "# Print the final prediction\n",
        "print(f\"The prediction is {labels[prediction]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mbA1ZOdtc3qj",
        "outputId": "603547e4-388e-4dea-8302-21ee7a5fc7d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            "I don't want to learn about Hugging Face Transformers!\n",
            "\n",
            "Tokenized Inputs:\n",
            "{\n",
            "    input_ids:\n",
            "        tensor([[    0,   100,   218,    75,   236,     7,  1532,    59, 30581,  3923,\n",
            "         12346, 34379,   328,     2]])\n",
            "    attention_mask:\n",
            "        tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
            "}\n",
            "\n",
            "Model Outputs:\n",
            "SequenceClassifierOutput(loss=None, logits=tensor([[ 3.9164, -3.5262]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "\n",
            "The prediction is NEGATIVE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sentiment Analysis with Padding:\n",
        "\n",
        "Imagine you have two sentences for sentiment analysis:\n",
        "\n",
        "Sentence 1: \"I had a great time at the park!\" (5 words)\n",
        "\n",
        "Sentence 2: \"The movie was so disappointing.\" (6 words)\n",
        "\n",
        "To process both sentences together in a batch, they need to be the same length. So, you add a padding token to Sentence 1.\n",
        "\n",
        "\n",
        "If the model doesn't have an attention mask:\n",
        "\n",
        "It analyzes all 6 tokens, including the padding in Sentence 1.\n",
        "\n",
        "This might confuse the model and lead to inaccurate sentiment prediction.\n",
        "With an attention mask:\n",
        "\n",
        "The mask is [1, 1, 1, 1, 1, 0] for Sentence 1.\n",
        "\n",
        "The model focuses only on the valid tokens (\"I had a great time\") and ignores the padding token.\n",
        "\n",
        "This leads to a more accurate positive sentiment prediction for Sentence 1."
      ],
      "metadata": {
        "id": "tKoIrr6dLjAV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = \"I'm excited to learn about Hugging Face Transformers!\"\n",
        "tokenized_inputs = tokenizer(inputs, return_tensors=\"pt\")\n",
        "outputs = model(**tokenized_inputs)\n",
        "\n",
        "labels = ['NEGATIVE', 'POSITIVE']\n",
        "prediction = torch.argmax(outputs.logits)\n",
        "\n",
        "\n",
        "print(\"Input:\")\n",
        "print(inputs)\n",
        "print()\n",
        "print(\"Tokenized Inputs:\")\n",
        "print_encoding(tokenized_inputs)\n",
        "print()\n",
        "print(\"Model Outputs:\")\n",
        "print(outputs)\n",
        "print()\n",
        "print(f\"The prediction is {labels[prediction]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NaC6NuIyc99a",
        "outputId": "5b673027-5b38-41ca-df3e-6ca7a658ae1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            "I'm excited to learn about Hugging Face Transformers!\n",
            "\n",
            "Tokenized Inputs:\n",
            "{\n",
            "    input_ids:\n",
            "        tensor([[    0,   100,   437,  2283,     7,  1532,    59, 30581,  3923, 12346,\n",
            "         34379,   328,     2]])\n",
            "    attention_mask:\n",
            "        tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
            "}\n",
            "\n",
            "Model Outputs:\n",
            "SequenceClassifierOutput(loss=None, logits=tensor([[-3.7605,  2.9262]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "\n",
            "The prediction is POSITIVE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "可以把不同模型依自己需要串起來，例如要做一個輸入相片得出一個故事的音檔，可以將三個模型：image to text, LLM, text to speech，三個模型串起來\n",
        "\n",
        "image to text：得出圖片的相關敘述\n",
        "\n",
        "LLM：一照圖片的文自描述產生故事\n",
        "\n",
        "text to speech：將得出的故事唸出來"
      ],
      "metadata": {
        "id": "wITiJV5B5g8I"
      }
    }
  ]
}